{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of users and time period for data generation\n",
    "num_users = 100\n",
    "num_days = 30\n",
    "start_time = datetime.now() - timedelta(days=30)\n",
    "end_time = datetime.now()\n",
    "time_range = pd.date_range(start=start_time, end=end_time, freq='H')\n",
    "\n",
    "\n",
    "# Generate user IDs\n",
    "user_ids = [f'user_{i}' for i in range(1, num_users + 1)]\n",
    "\n",
    "def generate_synthetic_metrics(user_id, timestamps):\n",
    "    data = {\n",
    "        'Timestamp': timestamps,\n",
    "        'UserId': user_id,\n",
    "        'CPUUtilization': np.random.normal(loc=50, scale=10, size=len(timestamps)),  # mean 50%, std dev 10%\n",
    "        'MemoryUtilization': np.random.normal(loc=70, scale=15, size=len(timestamps)),  # mean 70%, std dev 15%\n",
    "        'DiskIO': np.random.exponential(scale=100, size=len(timestamps)),  # exponential distribution\n",
    "        'NetworkIO': np.random.exponential(scale=200, size=len(timestamps)),  # exponential distribution\n",
    "        'InstanceUsageHours': np.random.poisson(lam=24, size=len(timestamps)),  # mean 24 hours\n",
    "        'DataTransferVolume': np.random.gamma(shape=2, scale=50, size=len(timestamps)),  # gamma distribution\n",
    "        'StorageUtilization': np.random.normal(loc=500, scale=50, size=len(timestamps))  # mean 500 GB, std dev 50 GB\n",
    "    }\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Generate data for all users\n",
    "all_data = pd.concat([generate_synthetic_metrics(user_id, time_range) for user_id in user_ids])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def introduce_anomalies(df, anomaly_rate=0.05):\n",
    "    num_anomalies = int(len(df) * anomaly_rate)\n",
    "    anomaly_indices = np.random.randint(1,72100, size=num_anomalies)\n",
    "    df.iloc[anomaly_indices,2] *=  np.random.uniform(1.5, 3.0, size=num_anomalies) #CPU spike\n",
    "\n",
    "    anomaly_indices = np.random.randint(1,72100, size=num_anomalies)\n",
    "    df.iloc[anomaly_indices,3] *= np.random.uniform(1.5, 3.0, size=num_anomalies)  # Memory spikes\n",
    "\n",
    "    anomaly_indices = np.random.randint(1,72100, size=num_anomalies)\n",
    "    df.iloc[anomaly_indices,4] *= np.random.uniform(1.5, 3.0, size=num_anomalies)  # Disk I/O spikes\n",
    "    \n",
    "    anomaly_indices = np.random.randint(1,72100, size=num_anomalies)\n",
    "    df.iloc[anomaly_indices,5] *= np.random.uniform(1.5, 3.0, size=num_anomalies)  # Network I/O spikes\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic data generation complete.\n"
     ]
    }
   ],
   "source": [
    "normal_data = all_data.copy()\n",
    "\n",
    "anomaly_data = introduce_anomalies(all_data)\n",
    "\n",
    "# Save to CSV\n",
    "normal_data.to_csv('synthetic_cloud_metrics.csv', index=False)\n",
    "\n",
    "anomaly_data.to_csv('synthetic_cloud_metrics_with_anoamaly.csv', index=False)\n",
    "\n",
    "\n",
    "print(\"Synthetic data generation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# # Load synthetic data\n",
    "# data = pd.read_csv('synthetic_cloud_metrics.csv')\n",
    "# X = data.drop(['Timestamp', \"UserId\"], axis=1)\n",
    "\n",
    "# # Train Isolation Forest model for anomaly detection\n",
    "# model = IsolationForest(contamination=0.05, random_state=42)\n",
    "# model.fit(X)\n",
    "\n",
    "# # Predict anomalies\n",
    "# predictions = model.predict(X)\n",
    "# data['anomaly'] = predictions\n",
    "\n",
    "# # -1 indicates anomaly, 1 indicates normal\n",
    "# anomalies = data[data['anomaly'] == -1]\n",
    "# print(anomalies.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.to_csv(\"anomaly.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate mean and standard deviation for each feature from normal data\n",
    "# normal_data = data[data['anomaly'] == 1]\n",
    "# normal_data.drop(columns=[\"UserId\", \"Timestamp\", \"anomaly\"], inplace=True)\n",
    "\n",
    "# normal_means = normal_data.mean()\n",
    "# normal_stds = normal_data.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to determine the cause of an anomaly\n",
    "# def determine_anomaly_cause(anomaly_row, normal_means, normal_stds):\n",
    "#     reasons = []\n",
    "#     for feature in anomaly_row.index:\n",
    "#         if feature in normal_means.index:\n",
    "#             deviation = abs(anomaly_row[feature] - normal_means[feature]) / normal_stds[feature]\n",
    "#             if deviation > 2:  # Adjust threshold as needed\n",
    "#                 reasons.append((feature, anomaly_row[feature], deviation))\n",
    "#     return reasons\n",
    "\n",
    "# # Apply the function to each anomaly\n",
    "# anomaly_reasons = anomalies.apply(determine_anomaly_cause, axis=1, args=(normal_means, normal_stds))\n",
    "# anomalies['reasons'] = anomaly_reasons\n",
    "\n",
    "# # Display the first few anomalies with reasons\n",
    "# print(anomalies[['Timestamp', 'UserId', 'anomaly', 'reasons']].head())\n",
    "# upper_thresholds = {\n",
    "#     'CPUUtilization': 80,\n",
    "#     'MemoryUtilization': 85,\n",
    "#     'DiskIO': 200,\n",
    "#     'NetworkIO': 400,\n",
    "#     'StorageUtilization': 600\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = normal_data.drop(['Timestamp', 'UserId'], axis=1)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_valid = train_test_split(X_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "X_test = anomaly_data.drop(['Timestamp', 'UserId'], axis=1)\n",
    "testing_df = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m1803/1803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 1.0353 - val_loss: 0.7764\n",
      "Epoch 2/50\n",
      "\u001b[1m1803/1803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.7669 - val_loss: 0.7379\n",
      "Epoch 3/50\n",
      "\u001b[1m1803/1803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.7317 - val_loss: 0.7174\n",
      "Epoch 4/50\n",
      "\u001b[1m1803/1803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.7169 - val_loss: 0.7080\n",
      "Epoch 5/50\n",
      "\u001b[1m1803/1803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.7107 - val_loss: 0.7032\n",
      "Epoch 6/50\n",
      "\u001b[1m1803/1803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.7031 - val_loss: 0.7003\n",
      "Epoch 7/50\n",
      "\u001b[1m1803/1803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.7041 - val_loss: 0.6983\n",
      "Epoch 8/50\n",
      "\u001b[1m1803/1803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.6967 - val_loss: 0.6969\n",
      "Epoch 9/50\n",
      "\u001b[1m1803/1803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.6943 - val_loss: 0.6958\n",
      "Epoch 10/50\n",
      "\u001b[1m1803/1803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.6922 - val_loss: 0.6949\n",
      "Epoch 11/50\n",
      "\u001b[1m1803/1803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.6959 - val_loss: 0.6940\n",
      "Epoch 12/50\n",
      "\u001b[1m1803/1803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.6931 - val_loss: 0.6933\n",
      "Epoch 13/50\n",
      "\u001b[1m1803/1803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 14/50\n",
      "\u001b[1m1803/1803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.6910 - val_loss: 0.6921\n",
      "Epoch 15/50\n",
      "\u001b[1m1803/1803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.6904 - val_loss: 0.6916\n",
      "Epoch 16/50\n",
      "\u001b[1m1803/1803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.6882 - val_loss: 0.6909\n",
      "Epoch 17/50\n",
      "\u001b[1m1803/1803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.6902 - val_loss: 0.6903\n",
      "Epoch 18/50\n",
      "\u001b[1m1803/1803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.6892 - val_loss: 0.6896\n",
      "Epoch 19/50\n",
      "\u001b[1m1803/1803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.6867 - val_loss: 0.6886\n",
      "Epoch 20/50\n",
      "\u001b[1m1803/1803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.6892 - val_loss: 0.6875\n",
      "Epoch 21/50\n",
      "\u001b[1m1803/1803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.6881 - val_loss: 0.6861\n",
      "Epoch 22/50\n",
      "\u001b[1m1803/1803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.6859 - val_loss: 0.6845\n",
      "Epoch 23/50\n",
      "\u001b[1m1803/1803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.6842 - val_loss: 0.6830\n",
      "Epoch 24/50\n",
      "\u001b[1m1803/1803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.6854 - val_loss: 0.6815\n",
      "Epoch 25/50\n",
      "\u001b[1m1803/1803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.6808 - val_loss: 0.6800\n",
      "Epoch 26/50\n",
      "\u001b[1m1803/1803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.6792 - val_loss: 0.6784\n",
      "Epoch 27/50\n",
      "\u001b[1m1803/1803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.6746 - val_loss: 0.6772\n",
      "Epoch 28/50\n",
      "\u001b[1m1803/1803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.6731 - val_loss: 0.6762\n",
      "Epoch 29/50\n",
      "\u001b[1m1803/1803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.6735 - val_loss: 0.6754\n",
      "Epoch 30/50\n",
      "\u001b[1m1803/1803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.6753 - val_loss: 0.6747\n",
      "Epoch 31/50\n",
      "\u001b[1m1803/1803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.6731 - val_loss: 0.6740\n",
      "Epoch 32/50\n",
      "\u001b[1m1803/1803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.6742 - val_loss: 0.6737\n",
      "Epoch 33/50\n",
      "\u001b[1m1803/1803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.6729 - val_loss: 0.6732\n",
      "Epoch 34/50\n",
      "\u001b[1m1803/1803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.6735 - val_loss: 0.6728\n",
      "Epoch 35/50\n",
      "\u001b[1m1803/1803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.6728 - val_loss: 0.6726\n",
      "Epoch 36/50\n",
      "\u001b[1m1803/1803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.6687 - val_loss: 0.6723\n",
      "Epoch 37/50\n",
      "\u001b[1m1803/1803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.6729 - val_loss: 0.6720\n",
      "Epoch 38/50\n",
      "\u001b[1m1803/1803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.6699 - val_loss: 0.6719\n",
      "Epoch 39/50\n",
      "\u001b[1m1803/1803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.6712 - val_loss: 0.6717\n",
      "Epoch 40/50\n",
      "\u001b[1m1803/1803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.6693 - val_loss: 0.6716\n",
      "Epoch 41/50\n",
      "\u001b[1m1803/1803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 0.6718 - val_loss: 0.6715\n",
      "Epoch 42/50\n",
      "\u001b[1m1803/1803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.6700 - val_loss: 0.6714\n",
      "Epoch 43/50\n",
      "\u001b[1m1803/1803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.6751 - val_loss: 0.6713\n",
      "Epoch 44/50\n",
      "\u001b[1m1803/1803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.6694 - val_loss: 0.6713\n",
      "Epoch 45/50\n",
      "\u001b[1m1803/1803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.6726 - val_loss: 0.6712\n",
      "Epoch 46/50\n",
      "\u001b[1m1803/1803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.6708 - val_loss: 0.6711\n",
      "Epoch 47/50\n",
      "\u001b[1m1803/1803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 0.6676 - val_loss: 0.6711\n",
      "Epoch 48/50\n",
      "\u001b[1m1803/1803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.6732 - val_loss: 0.6710\n",
      "Epoch 49/50\n",
      "\u001b[1m1803/1803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.6706 - val_loss: 0.6710\n",
      "Epoch 50/50\n",
      "\u001b[1m1803/1803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.6697 - val_loss: 0.6710\n"
     ]
    }
   ],
   "source": [
    "# Define the autoencoder model\n",
    "input_dim = X_train.shape[1]\n",
    "encoding_dim = 4\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "decoded = Dense(input_dim, activation='sigmoid')(encoded)\n",
    "\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the autoencoder\n",
    "history = autoencoder.fit(X_train, X_train, epochs=50, batch_size=32, validation_data=(X_valid, X_valid), verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to determine the cause of an anomaly based on upper thresholds\n",
    "def determine_anomaly_cause(anomaly_row, thresholds):\n",
    "    reasons = []\n",
    "    for feature in thresholds:\n",
    "        if anomaly_row[feature] > thresholds[feature]:\n",
    "            deviation = anomaly_row[feature] - thresholds[feature]\n",
    "            reasons.append((feature, anomaly_row[feature], deviation))\n",
    "    return reasons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_thresholds  = {\n",
    "    \"CPUUtilization\": 80,\n",
    "    \"MemoryUtilization\": 90,\n",
    "    \"DiskIO\": 300,\n",
    "    \"NetworkIO\": 500,\n",
    "    \"InstanceUsageHours\": 30,\n",
    "    \"DataTransferVolume\": 230,\n",
    "    \"StorageUtilization\": 600\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2254/2254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# X_pred = autoencoder.predict(testing_df)\n",
    "# reconstruction_error = np.mean(np.power(X_scaled - X_pred, 2), axis=1)\n",
    "# threshold = np.percentile(reconstruction_error, 95)\n",
    "# X_test['reconstruction_error'] = reconstruction_error\n",
    "# X_test['anomaly'] = np.where(X_test['reconstruction_error'] > threshold, -1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_anomalies(data, autoencoder, upper_thresholds, threshold=95):\n",
    "\n",
    "    X = data.copy()\n",
    "    predictions = autoencoder.predict(X)\n",
    "    reconstruction_error = np.mean(np.power(X_scaled - predictions, 2), axis=1)\n",
    "    threshold = np.percentile(reconstruction_error, threshold)\n",
    "    \n",
    "    X['reconstruction_error'] = reconstruction_error\n",
    "    X['anomaly'] = np.where(X['reconstruction_error'] > threshold, -1, 1)\n",
    "\n",
    "    anomalies = X[X['anomaly'] == -1]\n",
    "    anomaly_reasons = anomalies.apply(determine_anomaly_cause, axis=1, args=(upper_thresholds,))\n",
    "    anomalies['reasons'] = anomaly_reasons\n",
    "\n",
    "    # Filter out anomalies with no significant reasons\n",
    "    anomalies = anomalies[anomalies['reasons'].apply(len) > 0]\n",
    "    return anomalies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CPUUtilization', 'MemoryUtilization', 'DiskIO', 'NetworkIO',\n",
       "       'InstanceUsageHours', 'DataTransferVolume', 'StorageUtilization'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2254/2254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\riyac\\AppData\\Local\\Temp\\ipykernel_24268\\633353295.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  anomalies['reasons'] = anomaly_reasons\n"
     ]
    }
   ],
   "source": [
    "anomalies = predict_anomalies(X_test, autoencoder, upper_thresholds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pca of a\n",
    "# from sklearn.decomposition import PCA\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# pca = PCA(n_components=2)\n",
    "\n",
    "# X_pca = pca.fit_transform(data.drop(['Timestamp', 'UserId', 'anomaly'], axis=1))\n",
    "\n",
    "# plt.scatter(X_pca[:, 0], X_pca[:, 1], c=data['anomaly'], cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "ec2 = boto3.client('ec2', region_name='ap-south-1',\n",
    "    aws_access_key_id='AKIAU6GD2JDZCOGEI6NY',\n",
    "    aws_secret_access_key='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')\n",
    "\n",
    "\n",
    "instance_id = 'i-059a35694e6b4142a'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance i-059a35694e6b4142a stopped due to anomaly detected by ANomaly\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Stop the instance if an anomaly is detected\n",
    "ec2.stop_instances(InstanceIds=[instance_id])\n",
    "print(f\"Instance {instance_id} stopped due to anomaly detected by ANomaly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance i-059a35694e6b4142a is stopped\n"
     ]
    }
   ],
   "source": [
    "# check staus of EC2 instance\n",
    "\n",
    "response = ec2.describe_instances(InstanceIds=[instance_id])\n",
    "\n",
    "# Parse the instance state\n",
    "if 'Reservations' in response and len(response['Reservations']) > 0:\n",
    "    for instance in response['Reservations'][0]['Instances']:\n",
    "        state = instance['State']['Name']\n",
    "        print(f\"Instance {instance_id} is {state}\")\n",
    "else: \n",
    "    print(f\"Instance {instance_id} not found or no information available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upscale the EC2 instance\n",
    "\n",
    "# Stop the instance if an anomaly is detected\n",
    "ec2.stop_instances(InstanceIds=[instance_id])\n",
    "print(f\"Instance {instance_id} stopped due to anomaly detected by ANomaly\")\n",
    "\n",
    "# Define the new instance type\n",
    "new_instance_type = 't3.large' \n",
    "\n",
    "# Modify the instance attribute to change the instance type\n",
    "try:\n",
    "    response = ec2.modify_instance_attribute(\n",
    "        InstanceId=instance_id,\n",
    "        InstanceType={\n",
    "            'Value': new_instance_type\n",
    "        }\n",
    "    )\n",
    "    print(f\"Successfully scaled up instance {instance_id} to {new_instance_type}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error scaling up instance {instance_id}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
